
<!DOCTYPE html>
<html>
<head>
  <title>Multivariate distributions</title>
  <meta name="Multivariate distributions" content="text/html; charset=utf-8;" />
  <script type="text/javascript" src="../../../../logbook.js"></script>

  <script src="../../../../logbook-mathjax-config.js" defer></script> 
  <script type="text/javascript" id="MathJax-script" defer
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/atom-one-light.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.4.0/mermaid.min.js"></script>
  <script> mermaid.initialize({startOnLoad: true}); </script>  

  <link rel="stylesheet" type="text/css" href="../../../../logbook.css" />
</head>

<body onload="loadChapter('');">  

  <div data-type="titlepage" pdf="no">
    <header>
      <h1><a href="../../../../index.html" style="text-decoration:none;">Logbook</a></h1>
      <p style="font-size: 18px;"><a href="../../../../bio.html">Jayson Wynne-Thomas</a></p>
      <p style="font-size: 14px; text-align: right;"> 
        Last modified <span id="last_modified"></span>.</br>
        <script>
        var d = new Date(document.lastModified);
        document.getElementById("last_modified").innerHTML = d.getFullYear() + "-" + (d.getMonth()+1) + "-" + d.getDate();</script>
      </p>
    </header>
  </div>
  
  <div id="main" class="sidebar1">
    <span style="font-size:10px;cursor:pointer" onclick="openNav()">&#9776;</span>
  </div>

  <div id="mySidenav" class="sidebar">
  
<a href="#0">Intro</a>
<a href="#1">Scalar value functions</a>
<a href="#2">Vector value functions</a>
<a href="#3">Multivariate Gaussian and its parameter estimates</a>
<a href="#4">Example of a two dimensional covariance</a>
<a href="#5">Joint distribution</a>
<a href="#6">Marginal</a>
<a href="#7">Independence</a>
<a href="#8">Correlation</a>
<ul class="no-bullets">
  <li><a href="#8.0">Pearson Product-Moment Correlation</a></li>
  <li><a href="#8.1">Application of the correlation test</a></li>
</ul>
<a href="#9">Relationship of Correlation with Covariance</a>
<a href="#10">Conditional distribution</a>
<ul class="no-bullets">
  <li><a href="#10.0">Chain rule</a></li>
  <li><a href="#10.1">Bayes' rule</a></li>
</ul>
<a href="#11">Applying Bayes' rule</a>
<ul class="no-bullets">
  <li><a href="#11.0">Properties of the equation for the mean of the posterior distribution</a></li>
</ul>
</div>

<chapter style="counter-reset: chapter 0"><h1>Multivariate distributions</h1>

<section id="0"><h1>Intro</h1>
  Consider a random variable that is:
  <ul>
    <li>A vector and not a scalar</li>
    $x=[x_1,x_2,..,x_n]$

    <p>
      <li>Multi-dimensional</li>
      Considering x,y,z dimensions and attitude as well, an object's state vector becomes six dimensional.
    </p>
  </ul>
  
  The aim is to have a distribution over all the variables of a multi-dimensional vector. Treating each one separately, implicitly, makes a strong assumption about each one's independence.
  
  <p>
     The same concepts used for scalar values can also be used with vectors.
  </p>
</section>

<section id="1"><h1>Scalar value functions</h1>
  <ul>
    <li>Scalar Variable</li>
    $$
      x
    $$

    <li>Normalisation</li>
    $$
      \int^\infty_{-\infty} f(x)dx = 1
    $$
    $f(x)$ is the probability density function with a constant multiplier in front to make the integral go to 1. $f(x)dx$ gives the probability of $x$ for the interval $dx$.

    <p>
      <li>Mean</li>
      $$
        \mu = \int^\infty_{-\infty} xf(x)dx
      $$
    </p>

    <li>Variance or spread</li>
    $$
      \sigma^2 = \int^\infty_{-\infty} (x-\mu)^2f(x)dx
    $$
  </ul>
</section>

<section id="2"><h1>Vector value functions</h1>
  <ul>
    <li>Vector Variable</li>
    $$
      x = [x_1, x_2, \cdots, x_n]
    $$

    <li>Normalisation</li>
    $$
      \int^\infty_{-\infty} \cdots \int^\infty_{-\infty} f(x)dx_1 \cdots dx_n = 1
    $$

    <li>Mean</li>
    $$
      \mu = \int^\infty_{-\infty} \cdots \int^\infty_{-\infty} xf(x)dx_1 \cdots dx_n
    $$
  
    <li>Variance or spread</li>
    $$
      \Sigma = \int^\infty_{-\infty} \cdots \int^\infty_{-\infty} (x-\mu) (x-\mu)^T f(x)dx_1 \cdots dx_n
    $$

    Variance is replaced by a <i>covariance matrix</i>, $\Sigma$. It is a matrix because the outer product, $(x-\mu)(x-\mu)^T$, results in a matrix. 
    
    <p>
      The covariance matrix is going to be
      <ul>
        <li><i>symmetric</i> and</li>
        <li><i>positive semi definite</i></li>
        That is, there are no negative eigenvalues.
      </ul>
    </p>
  </ul> 
</section>

<section id="3"><h1>Multivariate Gaussian and its parameter estimates</h1>
  <figure>
    <img style="height:40%; width:30%"
    src="../../../../figures/drone/16_multivariateGaussian.png">
  </figure>

  The functional form is not very different from the scalar gaussian:
  <ul>
    <li>The square root of $2\pi$ is replaced with $\frac{n}{2}$ for a vector $x$ of length $n$.</li>
    <li>$|\Sigma|$ is the <code>determinant</code> of the covariance.</li>
    <li>
      Vector multiplication $(x-\mu)^T\Sigma^{-1}(x-\mu)$ is used.
    </li>
  </ul>
  
  The estimate of the parameters is almost the same as before.
</section>

<section id="4"><h1>Example of a two dimensional covariance</h1>
  <div class="container">
    <figure>
      <img style="height:200px; width:auto"
      src="../../../../figures/drone/17_multivariateGaussianExample.png">
      <figcaption>
        An overhead view. The center of the ellipse is the mean. Eigenvalues and eigenvectors, extracted from the covariance, give the 1st and 2nd eigen vectors. 
      </figcaption>
    </figure>
  </div>

  An important property of the covariance is that its <code>eigenvalues</code> and <code>eigenvectors</code> describe the amount and direction of uncertainty.

  <p>
    An ellipse drawn at one eigenvalue away from the mean along each vector, defines a confidence region which in this case is an ellipsoid.
  </p>
  
    Just as one standard deviation distance away from the mean in the scalar case defined the 68% confidence interval for x, the ellipse in 2D defines the confidence interval. The total probability inside the ellipse at 1 standard deviation is not 68% in this case. The total probability for a given length of axis actually depends on the number of variables in the random vector. But in 2D, 95% probability is captured by axis with length that is roughly five times the eigenvalues.
</section>  

<section id="5"><h1>Joint distribution</h1>
  A lot of times, knowing the value of one field in a multivariate vector tells something about the other fields. For instance, if a vehicle has a distribution over a state x, y, z, knowing the altitude of the vehicle may tell something about its lateral position or vice versa. 

  <p>
    Hence, given a multivariate distribution, it cannot be assumed that the probability distribution around each variable or field in the vector is independent.
  </p>
  
  For a vector $x = [x_1,x_2]$, and $p(x) = p(x_1, x_2)$, the function $p(x)$ is a probability distribution over the 2 variables that takes the vector as an input and outputs a scalar density.
</section>

<section id="6"><h1>Marginal</h1>
  To find the probability over $x_1$ from the joint distribution, the <i>marginal</i> is computed by integrating $x_2$ out of the density function:
  $$
    p(x_1) = \int{p(x_1,x_2)dx_2}
  $$

  <m>It is possible to integrate out all the variables that is not required, to get a marginal over any subset of variables that is required</m>.
</section>

<section id="7"><h1>Independence</h1>
  A system being estimated may have more than two variables. A flying vehicle may have variables for x, y, z, roll, pitch, yaw and perhaps even for their derivatives which takes it to 12. There might also be variables to track what's happening in the environment - example, states (12 each) of other drones. This distribution can get unwieldy very quickly.

  <p>
    But only some of the variables may actually have information about each other, i.e. some of the random variables in the vector are independent. <i>Independence</i> is the property that the joint probability density function is the product of individual densities of the component variables:
    $$
      p(x_1,x_2) = p(x_1)p(x_2)
    $$
  </p>
  <div class="container">
    <figure>
      <img style="height:80px; width:auto"
      src="../../../../figures/drone/20_notIndependent.png"/>
      <figcaption>
        Dependent
      </figcaption>
    </figure>

    <figure>
      <img style="height:80px; width:auto"
      src="../../../../figures/drone/21_independent.png"/>
      <figcaption>
        Independent
      </figcaption>
    </figure>
  </div>

  For a 2D Gaussian, if $x_1$ and $x_2$ are correlated, then the distribution is oval shaped and tilted. Here, for a positive $x_1$, it is very likely to have a positive $x_2$. For a negative $x_1$, it is very likely to have a negative $x_2$.
  
  <p>
    On the other hand, for a spherical Gaussian, knowing a value on the $x_1$ axis, tells very little about the values on the $x_2$ axis. 
  </p>

  The idea of independence is that knowing the value of one variable has no effect on the other random variable, i.e. knowing the value of $x_1$ tells nothing about $x_2$.
</section>

<section id="8"><h1>Correlation</h1>
  How to know if 2 variables are independent? How is it possible to tell if a random variable such as whether a vehicle is working is independent of whether its payload is working? 

  <p>
    If the data is discrete and the probability mass function is known, i.e. <i>categorical data</i> is available, then a <i>chi-square test</i>, which is a type of statistical test can be used to determine the degree to which the differences between variables are due to chance or if they're actually correlated in some way. For continuous data, their <i>correlation</i> can be examined.
  </p>

  <div class="container">
    <figure>
      <img style="height:80px; width:auto"
      src="../../../../figures/drone/24_correlation.png"/>
    </figure>
    <p>
      Intuitively, correlation gives some measure of how related two variables are. Consider a distribution is sampled over 2 variables ($x_1$, $x_2$). The correlation is positive if a large $x_1$ means $x_2$ is likely to be large as well. For <i>zero correlation</i>, the knowledge of $x_1$ would not really say anything about $x_2$.
    </p>
  </div>

  This is fine for a qualitative understanding of whether a correlation is positive, negative, or zero. But to precisely quantify a correlation, a correlation function is needed.

  <subsection id="8.0"><h1>Pearson Product-Moment Correlation</h1>
    There are many different choices for correlation functions. One common choice is the <i>Pearson Product-Moment Correlation</i>.
    $$
      \rho_{1,2} = \frac{cov(x_1,x_2)}{\rho_{x_1}\rho_{x_2}}
    $$
    $\rho_{x_1}\rho_{x_2}$ - Standard deviations of the variables
    <br>
    $\rho_{1,2}$ - Correlation between the variables
    <br>
    $cov(x_1,x_2)$ - Covariance of the 2 variables

    <p>
      If the data are independent, then the correlation is going to be equal to $0$ because the covariance is going to be 0 as follows:

      $$\begin{align*}
      cov(x_1, x_2)  &= E[x_1x_2] - E[x_1]E[x_2] \\
      if\:independent &= E[x_1]E[x_2]- E[x_1]E[x_2] \\
                     &= 0
      \end{align*}$$
    </p>
  </subsection>

  <subsection id="8.1"><h1>Application of the correlation test</h1>
    Imagine we had 2 random variables constructed from some other random variable that itself is distributed from $0$ to $2\pi$.

    $$\begin{align*}
      y_1 &= sin(x) \\
      y_2 &= cos(x)
    \end{align*}$$

    $x$ is uniform on $[0,2\pi]$.

    <p>
      The $y$ variables are clearly not independent - if we know one with high probability, we know the other. But, the expected value of all the $y$'s is equal to $0$, and the co-variance is equal to $0$. Which means the variables are uncorrelated, but they're clearly not independent.

      $$\begin{align*}
        E[y_1] = E[y_2] = E[y_1y_2] = 0 \\
        cov(y_1,y_2) = 0
      \end{align*}$$
    </p>

    For <n>jointly</n> Gaussian random variables $x_1$ and $x_2$, we can show that if the correlation is zero then the variables are indeed independent. It's a useful test just for this particular setting. It's also easy to construct an example where if the marginal of $x_1$ and the marginal of $x_2$ are each Gaussian, the covariance will be $0$ even though they're clearly not independent.
  </subsection>
</section>


<section id="9"><h1>Relationship of Correlation with Covariance</h1>
  When two random variables are <i>correlated</i>, knowledge of one provides some information about the other. For example, the position of a vehicle now and the position of a vehicle in one second are correlated random variables: knowledge of one should give me some insight into the value of the other.

  <p>
    The <i>Pearson correlation coefficient</i> is one way of quantifying this relationship.
  </p>
  
  <i>Covariance</i> is a measure of the joint variability of two random variables. If one variable tends to be big when the other is big, then the covariance is positive. The <i>correlation coefficient</i> is simply the <n>normalized version</n> of the covariance.
</section>

<section id="10"><h1>Conditional distribution</h1>
  Another useful property of joint random variables is that it's often of extreme interest to find a density of one of the unknown random variables given specific knowledge of other random variables. For instance, is a vehicle moving, given the current state of the propulsion system.

  <subsection id="10.0"><h1>Chain rule</h1>
    $$
      p(x|y) = \frac{p(x,y)}{p(y)}
    $$

    $p(x,y)$ is the joint distribution - probability of $x$ and $y$.
    <br>
    $p(x|y)$ is the <i>conditional distribution</i>.
    <br>
    $p(y)$ is the marginal of the conditioning variable.
  </subsection>

  <subsection id="10.1"><h1>Bayes' rule</h1>
    A really useful application is sometimes we want to reverse our conditional probabilities. When we do estimation for a flying vehicle, we generally have access to measurements $y$ (example: GPS which will always have some noise) and we want to know the probability of $x$ given those measurements. So we want to calculate the conditional probability $p(x|y)$.
    
    <p>
      If we had a model of the position probability density function given a measurement then we have everything we need. But it's often much easier to generate a model of the measurement distribution (i.e. generate the distribution of measurement likelihoods) given the current position, $p(y|x)$. This is where <i>Bayes' Rule</i> is really helpful! We can apply the chain rule twice:

      $$
        p(x|y) = \frac{p(y|x)p(x)}{p(y)}
      $$
    </p>
  </subsection>
</section>

<section id="11"><h1>Applying Bayes' rule</h1>
  How can we use Bayes' rule?
  <ul>
    <li>We start with <i>prior</i> belief</li>
    For example, imagine that we don't know where our vehicle is, but we have some knowledge about roughly where it is, called the prior. We know the vehicle position is distributed with some mean and some variance. The variance tells us how well we think we know where the vehicle is.

    <p>
      Let us assume that the prior distribution of $x$ is Gaussian with mean $\mu_x$ and variance $\sigma^2_x$:
      $$
        p(x) = \mathcal{N}(\mu_x, \sigma^2_x)
      $$
    </p>

    <li>We make a measurement $y=x+v$</li>
    $x$ is the true position of the vehicle, $v$ is the error added during measurement, and $y$ is the received measured value. We can assume that the position is independent of the measurement noise, which means $x$ is independent of $v$.

    $$
      p(v) = \mathcal{N}(0,\sigma^2_v)
    $$

    <p>
      <li>Compute $p(y|x)$</li>
      Given the prior and the measurement distributions are gaussian, the likelihood of the measurement $y$ given $x$ is also Gaussian. From chain rule, we can derive:
      $$\begin{align*}
        p(y) &= \mathcal{N}(\mu_x,\sigma^2_x + \sigma^2_v) \\
        p(y|x) &= \mathcal{N}(x, \sigma^2_v)
      \end{align*}$$
    </p>

    <li>Find the posterior distribution $p(x|y)$</li>
    We can compute the distribution over the position given the measurement using Bayes' rule. If we apply all the normal distributions into the Bayes' rule equation (product of two normal distributions, divided by a third normal distribution), we get another normal distribution as the result.

    $$
      p(x|y) \sim \mathcal{N}(\mu, \sigma^2)
    $$

    where the mean and variance are as follows:
    $$
      \mu = \mu_x + \frac{\sigma^2_x}{\sigma^2_x + \sigma^2_v}(y-\mu_x)
    $$

    $$
      \sigma^2 = \frac{\sigma^2_x\sigma^2_v}{\sigma^2_x + \sigma^2_v}
    $$
  </ul>

  <button class="accordion">Derivation of the posterior distribution $p(x|y)$ is a work in progress</button>
  <div class="panel">
    <p></p>
    The marginal probability of $y$:
    $$\begin{align*}
      p(y) &= \int p(y|x) \times p(x) dx \\
           &= \int \mathcal{N}(x, \sigma^2_v) \times \mathcal{N}(\mu_x, \sigma^2_x) dx
    \end{align*}$$

    $$\begin{align*}
      p(x|y) &= \frac{p(y|x) \times p(x)}{p(y)} \\
            &= \frac{\mathcal{N}(x, \sigma^2_v) \times \mathcal{N}(\mu_x, \sigma^2_x)}{\int \mathcal{N}(x, \sigma^2_v) \times \mathcal{N}(\mu_x, \sigma^2_x) dx} 
    \end{align*}$$

    $$
      \mu = E[x|y] = \frac{\sigma^2_x}{\sigma^2_x + \sigma^2_v} \times y + \frac{\sigma^2_v}{\sigma^2_x + \sigma^2_v} \times \mu_x
    $$
  </div>

  <subsection id="11.0"><h1>Properties of the equation for the mean of the posterior distribution</h1>
    $$
      \mu = \underbrace{\mu_x}_{\text{Initial estimate}} + 
      \underbrace{\frac{\sigma^2_x}{\sigma^2_x + \sigma^2_v}}_{\text{Trade off of trust} \atop \text{Measurement vs prior}}
      (\underbrace{y}_{\tiny{Measurement}}-\mu_x)
    $$
    <ul>
      <li>The measurement $y$ is applied as a correction to the initial estimate $\mu_x$</li>

      <p>
        <li>If $\sigma_v$ is small compared to $\sigma_x$, it means we have more trust in our measurement than in our initial estimate, and that results in a new estimate which is centered very close to the measurement</li>
      </p>

      <li>If we trust the prior more, i.e. $\sigma_x$ is small compared to $\sigma_v$, then the updated position estimate only gets moved slightly in the direction of the measurement</li>
    </ul>
  </subsection>
  
  
  
</section>
</chapter>

</body>
</html>
