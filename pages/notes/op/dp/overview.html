
<!DOCTYPE html>
<html>
<head>
  <title>DP: Overview</title>
  <meta name="DP: Overview" content="text/html; charset=utf-8;" />
  <script type="text/javascript" src="../../../../logbook.js"></script>

  <script src="../../../../logbook-mathjax-config.js" defer></script> 
  <script type="text/javascript" id="MathJax-script" defer
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/atom-one-light.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.4.0/mermaid.min.js"></script>
  <script> mermaid.initialize({startOnLoad: true}); </script>  

  <link rel="stylesheet" type="text/css" href="../../../../logbook.css" />
</head>

<body onload="loadChapter('');">  

  <div data-type="titlepage" pdf="no">
    <header>
      <h1><a href="../../../../index.html" style="text-decoration:none;">Logbook</a></h1>
      <p style="font-size: 18px;"><a href="../../../../bio.html">Jayson Wynne-Thomas</a></p>
      <p style="font-size: 14px; text-align: right;"> 
        Last modified <span id="last_modified"></span>.</br>
        <script>
        var d = new Date(document.lastModified);
        document.getElementById("last_modified").innerHTML = d.getFullYear() + "-" + (d.getMonth()+1) + "-" + d.getDate();</script>
      </p>
    </header>
  </div>
  
  <div id="main" class="sidebar1">
    <span style="font-size:10px;cursor:pointer" onclick="openNav()">&#9776;</span>
  </div>

  <div id="mySidenav" class="sidebar">
  
<a href="#0">Background</a>
<a href="#1">Description</a>
<a href="#2">Examples of the importance of the Cost-to-go function</a>
<a href="#3">Definitions</a>
<a href="#4">DP equation</a>
<ul class="no-bullets">
  <li><a href="#4.0">Non-recursive form</a></li>
  <li><a href="#4.1">Recursive form</a></li>
  <li><a href="#4.2">Optimality condition</a></li>
</ul>
<a href="#5">The algorithm</a>
<ul class="no-bullets">
  <li><a href="#5.0">Start</a></li>
  <li><a href="#5.1">Update</a></li>
</ul>
<a href="#6">Grid world</a>
</div>

<chapter style="counter-reset: chapter 0"><h1>DP: Overview</h1>
<section id="0"><h1>Background</h1>
  An <i>action</i> is a specific decision or choice made at a particular state of a problem. Actions are the individual steps taken by an agent to transition from one state to another. In dynamic programming, the choice of action at each state is typically guided by the policy, which determines the best course of action to achieve the desired objective.

  <p>
    A <i>policy</i> refers to a strategy or a rule that guides decision-making at each state of a problem. It defines what action to take at each state in order to maximize some objective, such as maximizing cumulative rewards or minimizing costs. A policy can be <i>deterministic</i>, meaning it prescribes a single action for each state, or <i>stochastic</i>, meaning it prescribes a probability distribution over actions for each state.
  </p>

  In general, an optimal policy is not unique - from a given state, it is possible to take 2 different optimal actions at still end up with the same cost. The optimal cost-to-go, however, is unique.

  <p>
    If a constant, say 10, is added to the cost-to-go at every node/cell, i.e. offsetting the cost-to-go, then the optimal cost-to-go will be wrong just in its value but the optimal policy will still be correct. This is what happens when initialising the cost-to-go with a random constant; it is not the case when initialising with 0.
  </p>
</section>

<section id="1"><h1>Description</h1>
  The algorithm used to solve the shortest path problem in computer science is <i>Dynamic programing</i>. The aim is to find the <i>shortest path</i> to a goal on a weighted directed graph, where the edges:
  <ul>
    <li>
      Are equivalent to taking different actions from the current state
    </li>
    <li>
      Have associated costs
    </li>
  </ul>

  In CS, DP is considered to be a recursive algorithm that solves backwards from a goal.

  <p>
    There are different graph search algorithms like Dijkstra's algorithm that solve from a particular initial condition (in the forward or backward direction) to the goal. DP is the algorithm that solves the graph simultaneously; it computes the optimal policy from all possible initial conditions. 
  </p>

  DP also has exact connections to the continuous time formulation.

  <p>
    The cost to go function is very important in controls - if the it can be found, then all the controller needs to do is to take actions to minimise it.
  </p>

  DP and Value iteration are used interchangeably, but here it is defined as follows:
  <ul>
    <li>DP: Finite time</li>
    <li><i>Value Iteration</i>: Infinite time or infinite horizon</li>
  </ul>
</section>

<section id="2"><h1>Examples of the importance of the Cost-to-go function</h1>
  <ul>
    <li>Connection with Lyapunov function</li>
    <li>An extension: Dissipation inequalities connected with robust control</li>
    <li>Able to make bounds on what happens to the Cost-to-go estimate if there is uncertainty</li>
  </ul>
</section>

<section id="3"><h1>Definitions</h1>
  <ul>
    <li>Discrete states</li>
    $$s_t \in S$$
    <li>
      Discrete actions
    </li>
    Finite set of edges or actions that can be taken from a state. 
    $$a_i \in A$$
    <li>
      Discrete time
    </li>
    Finding out the next state involves the system dynamics. 
    $$s[n+1] = f(s[n], a[n])$$

    <li>Cost function</li>
    <ul>
      <li>Edge cost</li>
      For a weighted shortest path problem, this is the cost of traversing some edge.
      $$g(s,a)$$

      <li>Total cost</li>
      The total cost of a trajectory is the accumulation of the 1 step costs.
      $$\sum_n g(s,a)$$
      The sum goes to infinite or finite time.

      <p>
        In optimisation theory, a trajectory can be scored in many ways. An example would be to score it as an additive cost along the trajectory. Let's say the cost function is scoring based on the time to goal, then:
        $$
          g(s,a) = \left\{
                    \begin{array}{ c l }
                      1 & \quad \text{if } s \neq s_{goal} \\
                      0 & \quad \text{otherwise}
                    \end{array}
                  \right.
        $$ 

        The cost keeps accumulating until the goal is reached. For the minimum time problem, the time to goal cost fits perfectly in DP's additive cost structure. 
      </p>
    </ul>
  </ul>
</section>

<section id="4"><h1>DP equation</h1>
  <subsection id="4.0"><h1>Non-recursive form</h1>
    $$
      J^*(s_i) = \min_{a[\cdot]} \sum^\infty_{n=0} g(s_i[n], a[n])
      \quad s[0] = s_i
    $$
    <ul>
      <li>$J(s_i)$</li>
      Cost to go (or value function using rewards instead which the language of RL). This is the cost that is expected to be incurred to get to the goal or to execute the remaining trajectory. The cost increases the farther the current state is from the goal. It is a function of state - from every node, the total cost to go can be computed.

      <p>
        <li>$J^*(s_i)$</li>
        If the optimal policy is followed from the current state, where the best possible actions are always taken to minimise the cost, then $J^*$ is the cost that would be incurred to get to the goal. 
      </p>

      <li>$\sum^\infty_{n=0} g(s_i[n], a[n])$</li>
        Accumulation of the costs over the trajectory that remains. <n>Ensure the cost converges for an infinite trajectory</n>.

      <p>
        <li>$\min_{a[\cdot]}$</li>
        Over the trajectory that remains, the action that returns the minimal <n>long term cost</n> is taken at every step. This is hard to search over since an infinite list of decisions (due to an infinite trajectory) might need to be made at every step.
      </p>
    </ul>
  </subsection>

  <subsection id="4.1"><h1>Recursive form</h1>
    $$
      J^*(s_i) = \min_a \left[g(s,a) + J^*(f(s,a)) \right]
    $$

    <ul>
      <li>$\min_a$</li>
      The search space is finite since the minimisation is performed only over all actions from the current state. The $a$ that minimises the above equation is the optimal action.

      <p>
        <li>$J^*(f(s,a))$</li>
        The optimal cost to go from the next step.

        $$
          J^*(f(s,a)) = \min_{a[\cdot]} \sum^\infty_{n=1} g(s_{i+1}[n], a[n])
        $$
      </p>

      <li>$g(s,a)$</li>
      The cost of taking a single step $a$.
    </ul> 
  </subsection>

  <subsection id="4.2"><h1>Optimality condition</h1>
    Given a graph with weights and edges, an optimal controller needs to have a certificate of optimality that guarantees that it will take the optimal path from every initial condition. This would also be a way to check for optimality.
    
    <p>
      The certificate has to satisfy the <i>self-consistency condition</i>, i.e. by taking the action suggested by the controller's policy under $J^*(s_i)$, $J^*(f(s,a))$ can be found. This would mean:

      <ul>
        <li>$J^*(s_i)$ is indeed optimal</li>
        <li>The policy is optimal since it gives the minimal action to take, i.e. the action suggested by the optimal controller is in fact $\min_a g(s,a)$</li>
      </ul>  
    </p>    

    The certificate also turns into an algorithm for finding the optimal controller. This is called the <i>Bellman equation</i>.
  </subsection>
</section>

<section id="5"><h1>The algorithm</h1>
  <subsection id="5.0"><h1>Start</h1>
    $\hat{J}^*$ is the estimate of the optimal cost-to-go. It is initialised to random values all over the graph. $J$ is just a vector.
  </subsection>

  <subsection id="5.1"><h1>Update</h1>
    On every iteration of the algorithm, as the estimates at various nodes/states get updated with the one step cost, plus the estimate of the future cost, $\hat{J}^*$ almost converges to $J^*$, i.e. $\hat{J}^* \rightarrow J^* + c$. Because of the way the update is formulated, there is a constant offset.

    $$
      \forall i \quad \hat{J}^*(s_i) = \min_a \left[g(s,a) + \hat{J}^*(f(s,a)) \right]
    $$

  </subsection>
</section>

<section id="6"><h1>Grid world</h1>
  The aim is for the bot to reach the goal in minimum time. There is a pit of death. The bot cannot move diagonally. Each grid cell can be considered to be a node of a graph. This is just a different representation of a graph search problem. 

  <p>
    The cost at every node can be written as:
    $$
      g(s,a) = \left\{
                  \begin{array}{ c l }
                    0 & \quad \text{at goal} \\
                    10 & \quad \text{at pit} \\
                    1 & \quad \text{otherwise} \\
                  \end{array}
                \right.
    $$
  </p>

  <div class="container">
    <figure2>
      <img style="height:160px; width:auto"
      src="../../../../figures/optimisation/1a_dp_grid.png"/>
      <figcaption>
        1st iteration
      </figcaption>
    </figure2>

    <figure2>
      <img style="height:160px; width:auto"
      src="../../../../figures/optimisation/1b_dp_grid.png"/>
      <figcaption>
        2nd iteration
      </figcaption>
    </figure2>

    <figure2>
      <img style="height:160px; width:auto"
      src="../../../../figures/optimisation/1c_dp_grid.png"/>
      <figcaption>
        3rd iteration
      </figcaption>
    </figure2>
  </div>

  <p>
    $J$ is assumed to be $0$ at every cell initially. At every iteration, it gets updated. After the first step, $J$ just becomes equal to the cost function at every cell. The knowledge of the path to the goal emerges with more iterations as costs propagate backwards and begin to converge. DP solves this recursively.
  </p>

  The grid can also be updated in a distributed manner. Here it was done in a sweep, but random states could be updated and as long as all states are visited with some probability, then the optimal policy can be found. This would be the case for a bunch of distributed robots making their own decisions (local updates) asynchronously, in a random order. The optimal c2g can still be found, up to a scalar, if all states are visited. 
</section>
58:20
</chapter>

</body>
</html>
