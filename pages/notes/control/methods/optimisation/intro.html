
<!DOCTYPE html>
<html>
<head>
  <title>PID control</title>
  <meta name="PID control" content="text/html; charset=utf-8;" />
  <script type="text/javascript" src="../../../../../logbook.js"></script>

  <script src="../../../../../logbook-mathjax-config.js" defer></script> 
  <script type="text/javascript" id="MathJax-script" defer
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/atom-one-light.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.4.0/mermaid.min.js"></script>
  <script> mermaid.initialize({startOnLoad: true}); </script>  

  <link rel="stylesheet" type="text/css" href="../../../../../logbook.css" />
</head>

<body onload="loadChapter('');">  

  <div data-type="titlepage" pdf="no">
    <header>
      <h1><a href="../../../../../index.html" style="text-decoration:none;">Logbook</a></h1>
      <p style="font-size: 18px;"><a href="../../../../../bio.html">Jayson Wynne-Thomas</a></p>
      <p style="font-size: 14px; text-align: right;"> 
        Last modified <span id="last_modified"></span>.</br>
        <script>
        var d = new Date(document.lastModified);
        document.getElementById("last_modified").innerHTML = d.getFullYear() + "-" + (d.getMonth()+1) + "-" + d.getDate();</script>
      </p>
    </header>
  </div>
  
  <div id="main" class="sidebar1">
    <span style="font-size:10px;cursor:pointer" onclick="openNav()">&#9776;</span>
  </div>

  <div id="mySidenav" class="sidebar">
  
<a href="#0">Overview</a>
</div>

<chapter style="counter-reset: chapter 0"><h1>Introduction</h1>
<section id="0"><h1>Overview</h1>
  Philosophy and implications of thinking of control as an optimisation problem. One of the simplest examples where the optimization problem can be solved exactly is the double integrator. What is the the numerical recipe for DP? What is the intuition about what it means to have a value function or a dynamic programming solution or a cost to go function. Proof that these have satisfying solutions to various non linear dynamical problems.
</section>

let me remind you you know the Control control problem that we started with last time so I spent a lot of the lecture I centered it sort of around the simple pendulum right in a particular we use the equations of motion like this and we started with a one dimensional visualization graphical analysis of what this thing could do and end it up at the end drawing the phase portrait theta versus theta dot which if you plot the solutions of the pendulum meaning if I started some initial condition on this some initial theta some theta dot I can watch the system evolve and it has it goes through if the system is undamped it goes through periodic motion where it just goes back and forth forever in these big cycles in state space they're small cycles if they started close to the origin and they get increasingly elongated here punctuated by these the unstable equilibrium and then the whole thing repeats again over here there's another copy of the fixed point over there right and those those are the periodic cycles of the undamped pendulum and similarly a damped pendulum has very similar trajectories but they start and they'll slowly wind down towards equilibrium ok so what I want to think about here is how do we actually control this system so if you think about the way that these plots I remember I also talked about how do you draw a vector on top of this right the every year to draw the tangent to that line at every point that's a vector field of the system so what ability do we have I the way I want you to think about control in this class is that you're given the world gives you some vector field right it gives you some dynamics it can be beautiful it can do it almost what you want or it could do something completely wrong and your goal is to change the vector field not ideally by erasing it and drawing your own vector field in but by somehow just doing a little bit of adjustment here and there you know to try to change this vector field into something that that does your will right so in the extreme case now see I get to go to the deep stack here of chalk we talked about one idea which was feedback linearization right so one idea for control would be let's call it feedback cancel is cancellation so imagine if I wrote my controller to be the following so if I said that I'm gonna choose you to be to MGL sine theta that's certainly something I could type in right I measure theta I apply you then what does that do if I put that into my equations of motion here that effectively replaces my right so the two MGL sine theta will cancel this that it basically changes this to a minus sign so that's gonna be changing gravity and turning it upside down that controller is the one that makes the normal pendulum act like it's an upside down pendulum and it only to act exactly like that so what is that doing in the vector field right it somehow has to be changing the whole vector field to have the eyeball over here right it's sorry for my lack of symmetry there but I've moved the stable fixed point from here well this is you know and we talked about different definitions of stability but I've moved it over to here so there's times where I had to change the vector quite of dramatically to do that right it takes a lot of control effort to change one vector field into another vector field that's just completely arbitrary in this case let's think even a little bit more so what do you actually get to do second order systems are the the most of this class is about second order systems and in second order systems you can't actually arbitrarily move the vector field right in fact the X component of the vector field of the vector I draw everywhere he's always just theta dot right the this is a point that starts at theta theta dot and the magnet and the magnitude of the vector is theta dot theta double dot so I'm allowed to change the Y component through control actions I have the ability to affect theta double dot but I don't actually have the ability to affect data dot so the only thing I actually get to do with control is to move add in some component and move that vector up and down I can't actually go this way around a state a phase portrait right the phase portrait you're always gonna go clockwise whether you like it or not okay that's just because it's a second-order system but I can clearly this is a controller that does it I can change my original eyeball into this eyeball given I have sufficient torque right so what's the peak torque required to implement that controller to MGL right and if I don't if I don't have to mg/l then what's gonna happen is if I if I were to let's say then maybe a model of that could be that I somehow saturate I try to do two mg l sine-theta but I saturated that I'm sorry to write almost pseudocode here your head you know negative 1 positive 1 or something like this if I didn't have enough torque and I ended up just clipping you know I try to ask for 10 and I only get one out then what's gonna happen I'm not going to be able to get this new vector field in fact when it's when you put it here you might just we know from the first-order analysis if I don't have enough torque it's just gonna get stuck here trying to go to the top but it'll it'll be stuck okay so torque limits are real you know we see robots walking around like this where they're working very hard to cancel their dynamics but you know as emotes not capable of doing a backflip and the interesting thing is once you impose a constraint like that the challenge even for the pendulum of stabilizing that fixed point it's actually pretty subtle if I want to somehow change the vector field I'm only allowed to modulate it a little bit I've got torque limits how do you rewrite this vector field and turn it in so this becomes the stable fixed point if you want to stabilize up here it's not trivial right I mean you can imagine what you have to do you have to write some controller that kind of goes pumps up energy like you're pushing your kid on a swing set right and then eventually when it gets up to the top it needs to regulate you can do it it's certainly possible with small amounts of torque but there's a circuitous route required to get yourself from some location over here to here you don't get to just go like that okay so as soon as you have torque limits even though this has got one degree of freedom one actuator so by the rank constraint definition this is this is fine but because of the saturations the systems under actuated and it becomes incredibly subtle actually to to program alright so today we're gonna try to give our first general algorithm I don't just want to we were to end the class and we could only control pendula I'd be disappointed but we want to give you a general recipe that's going to work for this system where we can understand it and it's going to work for much more complicated systems too good Optimization so so the we're gonna use a lot of tools from optimization in this class both optimization theory and numerical optimization so what would it mean to somehow specify the control problem as an optimization problem right what we mean is if I there's a there's numbers of there's a number of ways it's subtle versions of this but roughly what I want to say is if I watch the robot perform for some amount of time let me call that the trajectory X dot a new dot where I'm gonna mean this is this will be my shorthand for for all T X of T with T in 0 to some upper limit may be infinity maybe some T max right T max for now look at both okay but I'd like this to be an entire my shorthand for an entire trajectory okay so if if you program to control the robot to do something and I observe its motion in X and the controls you gave it in you for some amount of time possibly this time goes to infinity I need to assay for every time I watch it I'm going to give you a score single scalar number okay a reward if you will or a cost so if you're a reinforcement learning person you call it a reward and you give positive reinforcement if your control theorist you penalize people and you caught you give cost learning people like to optimize you know the reward and control people try to minimize the cost it's like ah I guess the learning people are winning in terms of popularity so I guess that was maybe that was the first mistake and in fact the today's lecture you could call it a reinforcement learning lecture but but that would not do justice to the long history of control results that have that started this whole thing okay so every time I watch the robot for some amount of time I need to be able to score it okay and then so some examples for instance for the pendulum maybe you'd like to say how long did it take from my system to get from some initial condition to the goal right would be it that be a reasonable scoring function maybe the average distance over the entire trajectory all right if I ask my pendulum for different things if I say get to the goal as fast as possible meaning I'm gonna penalize you more if it takes longer to get to the gold and it's gonna be right at its limits and try to get to the top as fast as possible if you want to if you give some you know there's other other cost functions might take a more gentle path for instance so this is our cost function our reward function our objective more generally in optimization we can also apply constraints so for instance if I said I have a torque limit something like this or maybe I say that I'd like to only I'd like to make sure that a time time final I my state is definitely at some goal state right one way to read this now is that I'd like to only consider trajectories which satisfy these constraints and over the set of trajectories that satisfy these constraints I'd like to find the best one by my score right it turns out to be both very powerful in general but often subtle and how do you specify these costs for more complicated tasks like if you ask me you remember I showed you the shoelaces find the shoelaces and it's like wow that I well there's a there's an initial I don't actually know what cost function I would use to try to derive to teach a robot to to tie my shoes or to make a salad or like you know cook dinner or something like that I don't know what the cost function is people say maybe maybe it's how similar do you look from a video you saw on YouTube who knows look we're going to simmer to see how that goes but for the problems here there's often that we're going to use in this class there's often gonna be relatively straightforward ways to specify the cost function if you want to do a back flip then maybe you're like set to go over your foot at some point and then you want to be back on the ground standing up so your legs had to go over your head at some point okay so let's let's put this to work in a one of Optimization Example a simple example where we can actually think about what optimization means for this okay the classical example that you use to get your head around this is the minimum time problem the double integrator so my equations of motion are just Q double dot equals you that's my double integrator but to make it interesting I'm gonna say you can't make it well posed for the double minimum time problem I'm gonna say that use is going to be restricted to be between negative 1 and 1 okay if you want a physical realization of this which I offer always like to think about what does it mean physically then if Q is my is a distance along some track for instance and I have a brick that has mass 1 and I'm pushing it with force you okay my my formulation is going to be to try to get me to some zero location on the on the axis in as little time as possible so [Applause] and by the way you want to be stopped at the goal you can't just pass through it [Applause] [Applause] Intuition okay so what do you think for those of you that have already thought about this problem maybe don't speak up but for those of you that are thinking about it for the first time what do you think would be the optimal actions without what's the best controller I could possibly write for this yeah exactly it's gonna be like Boston driving right you hit the gas as much as you can you go as fast as you possibly can until the last possible instant where you slam on the brakes and you come skidding into your parking spot right okay so that that's our intuition and that's gonna turn out to be optimal it's gonna be some how it's called the baying baying policy that's that's a general optimal control thing when you're you're slamming on the limits at all times it's a non-smooth controller right because you good you could already be well go so we're gonna see exactly yeah there could be states where your initial conditions were such that you have to hit the brakes yeah but if you started from stopped off here then yes good all right so so let's try to write this now as a controller not so that was our intuition from like a stopped initial condition off to the side but how do we generalize that so we can write it for as a function right ultimately what I want is to say whatever Q and Q dot I'm in right now I need to decide am i accelerating or am i decelerate right it's a completely spell it out so I'll call it a policy policy is control those are synonyms basically cost function reward function are I guess a antonyms but you know they're they're just a negative sign away from each other and then we'll talk about value functions and cost to go functions okay so to think about this let's just understand what happens if I'm in one side of this so if I have the system cued up without equals U and I'm applying U equals negative one then what is that what is that going to look like in the motions you know what does my phase ports are gonna look like if I'm applying this hit the brakes kind of controller okay this is there's not many differential collisions we talked about in class that I can integrate but this one I can integrate right this one is Q dot of T is gonna be few dot 0 minus T because it's minus 1 and Q of T is gonna be Q 0 of T plus Q dot t minus 1/2 a T squared when a was yet well plus 1/2 a T squared but he was negative 1 so okay so what does that look like if I plot it on this but let's say I started from zero zero and I hit used this policy it's not just a breaking policy it's a push this way as possible as possible and if I started at the origin I'm gonna get a curve that looks like heyou increases quadratically with time q dot on the increases linearly with time and in fact I just use that policy I'll move away like a like this okay yeah I get a parabola in this direction and similarly if I just shift qs0 I'm gonna get I can plot a lot of places on my face portrait pretty quickly here if I were to start with Q dot being higher then I get actually the same thing but I I come down in that parabola here like if I were to draw it this way it's the parabola so in fact it's just completing this these curves out this way those are just the parabolas like this okay similarly if I were to do the Q equals positive one policy are to hit you know push it on push it to the right the entire time and I'll get trajectories that look like this okay so this is what happens in those two extreme situations and what we're saying is that there's somehow some combination of pushing left or pushing right that in the end is going to be an optimal controller which from every additional mission tells me how to get to the goal as fast as possible what does it look like so if I start here what do I do blue or red gonna take blue to get here but then I'm gonna have to go around in order to come back right in fact the key the key trajectories here are this one here that's the magical one where if I start if I find my initial conditions on this line on this orbit then I will if I keep that pushing back this way that I will come to a stop exactly at the origin similarly this line is magical okay if I start here and I hit I use this u+ one that I'll get to the origin okay and in fact the optimal policy is exactly given by that okay so over here I want to choose everywhere I'm over here I'm gonna do you equals negative one and everywhere I am over here that was a really bad a little bit okay yeah this is positive this is negative thank you okay so I'm red over here and I'm blue over here okay and the paths that I take to the origin are actually not fairly non-trivial right so I take some trajectory I'm gonna actually go down to here and then ride it in okay but from any initial condition our intuition tells us that this is the best I can possibly do I'm living on the rails I'm always going in the direction I need to be going and I'll get there as fast as possible okay so that's sort of our minimum time policy okay for the double integrator it's pretty simple you can imagine even for the pendulum it gets pretty complicated so how the heck are we gonna come up with that kind of logic for arbitrary systems right how would we possibly get that from an algorithm well we're gonna use dynamic programming are there any questions about that part of that hole yes good absolutely but it's unavoidable the states where you are going to overshoot are the ones where you could not have Minimum Time decelerated fast enough to avoid the overshoot great question I mentioned a little bit about the teaching philosophy when I'm in the first lecture one of them is I score myself down if I monologue the whole time okay so help me out and ask a bunch of questions it makes it way more interesting for everybody I think okay so how can we possibly get that from an algorithm so it turns out I like minimum time as an optimization problem because it's very closely related to something we know in computer science which is just shortest path on a graph okay okay so if I had just wanted to do graph search train I have some directed graph okay and I have some gold maybe and I want to find the shortest path to the goal and the edges are equivalent to taking different actions from the goal then that becomes a shortest path problem in graph search a standard standard fare for computer science okay Dynamic Programming what I want to think about here is can we use that to solve our double integrator problem right and the answer is gonna be of course we can if we get a little sloppy about the beautiful dynamics and we approximated in a finite way with a bunch of grid points okay like having a graph lined up here where I have edges that go like this but I'm never I don't have any edges that go that way for instance right I just have a small number of edges that I can take and those represent my different control decisions right you equals maybe positive one negative one zero something like that now there's some subtleties you should already be thinking okay well probably I didn't land exactly in the bin right in this state that there's some approximations we have to make in order to go from the continuous state to this discrete grid okay but the tools of graph search can be used to approximate the solution to the optimal problems okay you know more generally you can do weighted shortest paths so if these had edges and I wanted to solve a shortest path problem on a weighted graph where I have some cost of going over each edge right 0.596 you know 11 whatever I can solve shortest I can weighted source shortest path problems okay and the solution that you use to do this in the the one of the main strands that you used to do this in computer science is dynamic programming that algorithm has exact analogues to this and even to the continuous form of it okay so we're gonna start in the discrete version I try to be careful about my notation I'll use X for state when it's continuous but I'll use s for state when it's discrete I'm going to use discrete actions to to start which are the edges in my graph here so there's a finite library of actions I can take or edges and discrete dynamics or discrete time so I'm gonna write my dynamics guess of n plus 1 is a function of s of n my cost function if I think of it as a weighted shortest path problem is gonna be an edge cost again I'm a little bit more of a control than reinforcement these days so I'm gonna use cost instead of reward sorry that'll be the the cost of traversing some edge and that I'll say my total cost of some trajectory is the accumulation of those 1 step costs and we'll talk about whether that's a infinite or ni time in a minute so already we've made some pretty big assumption about the cost in the general idea of control as an optimization we could have any you know you can score a trajectory in any possible way in this particular we're gonna score it as an additive cost this is a key idea here that I'm gonna write a total cost as the accumulation of independent of set of simple costs along the trajectory and that is going to be essential for the dynamic programming algorithm to Excel that adds a that gives you a recursive structure and the solution that we can exploit heavily okay and so if you if you can in your you know imagining what makes a good controller or not I mean watching YouTube is harder to write this way probably but you know doing a backflip I think you can write that way it's Additive Cost actually quite general you don't have to you know so so for instance the amount of time it takes to get to the goal you might think that's a that's not an additive cost but actually it could be if I just did you know time to goal for instance what would it be a reasonable way to write that yeah but how do I write it for just G what's G in that case thank you perfect one if X is not equal backs goal for instance s is not equal a school today zero otherwise right so I'm just gonna charge charge up my my cost everywhere until I get to the goal and that's a minimum time cost that fits in this additive cost structure perfect and of course if there's a lot more imaginative things you can do to just a forecast the additive cost in the continuous time of course will look like G of Si for instance DT and some integral but in the in the discrete time it's just a sum okay all right so given that the dynamic programming algorithm heavily leverages that additive structure Cost to Go so recursive algorithm it solves we don't we in computer science we think of it as solving backwards from the goal and that's going to be fine to start now so we're gonna write to find this super important term this is the cost to go it's again value function this typically means positive it's related to just basically the cost versus reward version of value function which is the language of reinforcement learning okay I'm gonna just find the cost to go is if I'm in some state how much how much costs do I expect to incur until I've gotten to the goal or till I finished my trajectory okay so in general that means over the trajectory that remains of my you know the long-term actions it's gonna be the accumulation of these costs right if I'm a long way from the goal I'd expect to have a high cost to go it's a function of state right for every edge for every node I have some expert I have some compel something I can compute which is the total cost I expect if I were to follow the optimal policy they would make the best possible actions to minimize my cost then this is the cost I would incur to get to the goal now this is a lousy hard thing to search over right that's a possibly infinite list of decisions I have to make and by the way let's be careful every time I write a sum like this you know every time I write something like this make sure it will converge let's say watch for convergence here okay so the big idea and dynamic programming is that we can do better than this this is a hard to search over a infinite trajectory but I can write it as a recursive algorithm okay the key observation is with that additive cost structure I can always write my cost to go from state si is the cost of taking one step a single a my one step cost plus the cost to go from the next step now this becomes an easy thing this is now just searching over a single a I can just this minimization we're going to do more advancement of optimization in class you don't need any clever optimization for this you can just enumerate all the possible a's and take the smallest one because it's a finite set okay for every state s i'll look at the one step cost plus this thing I've computed for the next state alright and this will if I can solve for this and find a J that is consistent with this then then I then I this is then this is the cost to go and it turns out the a that minimizes this is the optimal action I hope that I hope this equation is not mysterious this is just pulling of my son this is just pulling n equals zero out the front and then having this is just n from 1 to infinity this is just the rest of that song it's just pulling the first term out ok so this is a this is a check right this is a this is a condition to certify Certification optimality okay basically if I give you a graph with weights and the edges and everything and you say I've got a controller that's optimal okay that's gonna take the the optimal path from every initial condition the way you can convince me that you're correct is you establish the certificate which is you show me your J you show me your cost to go it needs to satisfy this self-consistency condition they're taking one step I get then I get J back okay that means it's it's a good J and your policy was good if it's the minimizing one in this case all right so this is really just a condition on optimality it's a way to check for optimality what's amazing about this though is that it also turns into an algorithm for finding the optimal controller okay in particular if I just start with a even a random Jane okay yeah oh this is so this is called the bellman equation so if our goal is to compute the policy from all possible initial conditions then that's a little different than what Dijkstra you think of as typically well you can have a more general view of extra there are a lot of graph search algorithms that solve from a particular initial condition and try to find forward or backward the ways to the goal dynamic programming is the is the that sort of solves the entire thing simultaneously yeah and it has exact to the continuous-time formulation that's the real reason why I picked it yeah we're going to come back to some of the discrete graph search for in the future but this is I think my shortest path to the continuous versions okay so algorithm I'm gonna guess start with a J hat this is my estimate no even called estimate of the optimal cost now J is just a you could think of it as a vector even because it's just there's a list of J's at every node okay if I start that with just completely random garbage and then I apply the bellman equation sorry I I just called it the bellman equation now you stuck it in my head all right if I just repeat this as an algorithm I improve my update by just recursively saying I'm gonna use the one step cost plus my guests of the future cost you can imagine now that's just an algorithm on every loop of the algorithm maybe I said I go through all of my sis and make this update then something amazing happens J hat star will converge to J star almost you get a way I've written it these you can get a constant term a constant offset okay you can start with almost random garbage just run this over and over again he will converge on the optimal cost to go there's a bunch of different names for it this is a dynamic programming algorithm also called value iteration I tend to I don't think it's a hard fast rule unfortunately but I tend to use dynamic programming for problems that have finite time and value duration where the goal is to solve for time goes to infinity the fun of the infinite horizon version but mostly those words are basically interchangeable okay so this cost to go function is going to be one of the most fundamental ideas in really in controls okay it's easy to think about I think on the graph okay but it turns out to be really deep and important across controls if you can find a cost to go function or even a very approximate cost to go function then what have you found you found some something that sort of tells you how to act right in fact if I if I have this cost to go function then I can I can decide how to act by just trying to figure out how to go down on my cost to go function okay so there are lots of good ways to visualize that but because it's so incredibly important I just want to want to make sure you're thinking about it in the right way there's gonna be connections between cost to go functions and Lyapunov functions there's gonna be extensions that are called dissipation inequalities that talk about robust control you're gonna be able to make bounds on what happens if uncertainty enters you have approximate cost agos it's it becomes really really fundamental so I want you to think a lot about what this cost to go means so the one that everybody draws when they first start thinking about cost to go is I'm sorry to use the textbook example here but is you have a grid world you have a happy little robot in your grid world okay moves up down left or right and then there's you know some gold over here and there's some like massive pit of death over here okay and you'd like to navigate your your robot from any possible initial condition to the goal for instance in minimum time or you could sprinkle rewards around here right this is a you can imagine mapping this to a graph right if I just started connecting these edges right there's like there's a graph underlying this that looks just like my graph search problem I've just drawn it a slightly different representation okay so my actions are discrete I've got the four directions some some versions of it you're allowed to move diagonally I'll just do it for here although in the code I actually did the diagonal okay I'll give my cost just like G s a is zero and gold and one everywhere else so I guess standing still and not going towards the goal for ten seconds is equivalent to falling into the pit of despair although maybe you can't transition out all right so what is the value iteration or this dynamic programming algorithm do on this problem I just want you to have like an intuitive sense this is the place where we can really really see what should it do all right let's start not with the random numbers but let's just start let's just say J is zero everywhere okay my initial guess is that it's zero everywhere I'm gonna go through and I'm gonna update every cell just once what's what is my first my next round of cost to go estimates good so actually no it's that's tempting i but so certainly this one is going to get a 1 because I've taken some amount of time to transition here I get a 1 here I completely agree with that but what is this one gonna be I think it's gonna be 1 right if I have to transition exactly one G of Si there's no action here they give me zero cost I've done that before too but I think actually everywhere it gets a 1 the best thing I can do here is to move somewhere and incur one thing of cost right so that I get ones everywhere and except for the tens in here for instance if I could just get my cost function back after one step okay on the next round of the algorithm this one can continue to only get one because there's a path which cost me one to a goal of zero all right this was zero always but on the second round of the algorithm this becomes a 2 and actually I flood the whole space with twos okay on the next round of the algorithm I'm not a jock but I get threes in the places that are three away from the goal right and I'll eventually back out as you can imagine it takes time for the zero to the knowledge if you will of being able to have a path to the goal where cost stops accumulating yeah you're right I didn't I put it to there too quickly the two is here good those stay thank you for catching that those stay one these become too but those are all one and when I'm done when I'm all done these stay - all the things that are two steps away from the goal and then there's a ring of three right and the algorithm will just propagate back out the beauty of it is it solves this recursively and it will converge at some point the change that'll make no changes right and that solution will be the added cost to go the true cost to go okay here's a little animation of my pit of death my goal and I'm going to solve Pit of Death the you know the optimal cost to go but also you see the whole thing floods with color as the cost goes up and up and up and iterations of the algorithm and I've also drawn an arrow which is the minimum action that would take as I go through there but this is really see it's good to go through the examples like this because for instance is the optimal action unique if I have a value function right if I'm here I could have gone there or I could have gone there and I paid the same cost right in general actually optimal policies are often not unique is the optimal cost to go unique yes okay there's only one cost to go that is the true optimal cost to go the interesting thing is though if I take my optimal cost to go and add 10 to it everywhere okay just lift the whole thing up then it will lie to me about how much cost is remaining but actually the optimal policy is still correct right so the fact that this algorithm is off by a constant if I start with random initial junk actually if I start with 0 to be fine but I started with random junk than it actually converges off up to a constant okay now the magical thing about this algorithm is just really good and so you can do this a synchronously you can if you have like a bunch of distributed robots and they're all making their own little local updates to this you can you can update some s's and then update another s and then whatever it's still all this stuff there so there's pretty deep results that say that this is a contracting a contractor there's a contraction metric which says that that thing you can abuse it and it will almost always still find the optimal cost to go up to a scalar okay so I did it in a sweep where you go through and update everything every time and then you go through the next set but you could just pick on each step you could just pick a state randomly update that one pick another state randomly as long as you visit them all with some probability then you will actually find the optimal policy okay so now let's do this double integrator thing okay so let's see what this algorithm would give me if I were to put it on top of a discretized version of the double integrator okay so I did some work to set up to call a value iteration algorithm I set up my little actually most of the code here is just drawing and making sure that it pauses and shows you what it's doing in the middle and and then a simulation at the end to show us how the controller does okay so here's a minimum time cost for the double integrator it's basically what we just said I just was because I have a continuous X in my equations that I'm trying to make the leap I just said if X is x squared is less than 0.05 that means I'm at the origin I get zero cost otherwise I get a cost of one everywhere okay and then I also there's some work up here to say I've made some bins in the Q direction I'm gonna make 31 bins from between negative 3 and 3 and 51 bins in the in the Q direction okay I'm gonna have my inputs go from negative 1 to 1 I'll make nine of them ok even though we expect probably that it'll only use two of them when we're done let's go ahead and give it the richness to do nine of them and then given this cost function let's run the value iteration algorithm okay I'm trying to convert everything to the notebook workflow and it's almost always good every once in a while I just feel like I can't see you like it does things like that and I hate it but that's probably more me than it okay so what just happened here this is my cost to go function this is the policy right the same was I was able to compute the arrows at the same time okay so well does that look familiar I think I can get my colors right so there's some dis critias Asian artifacts there okay but actually it finds the solution that we'd expect actually we'll probably ask you we haven't I'm not sure if you've decided if we're gonna ask for not there's a question we sometimes I ask on the problem set about what is actually the difference between the discrete solution and the continuous solution it's actually wrong characteristically it tends to undershoot the you know it gets the curve it's against a curve that looks similar but it's in the wrong place systematically because of some of the numerical artifacts and well maybe ask you to look at that okay and the it's really super useful the same way you want to know how far you are from the goal or how close you are to the pit of death whatever to look at the cost to go function again for for this double integrator you can look at that above from above to and it has actually a lot of the same geometry as this the cost ago you can compute this in the notes we actually compute write out the expression for T it's not super pretty because there's cases and stuff like this but it goes up here you'd expect to have a little bit more cost here than here okay and it turns out to have this minimum trough along that axis which is true okay what's the red stuff on the edge why does it kind of why would it flatten out like that that's a failure of my code that's a boundary condition artifact I don't know how to initialize the boundaries to be at the optimal solution a priori so it thinks might in by discrete approximation it actually can't right there's a path that it should be able to take that goes off here and comes back but since I've truncated it it's slamming into this and going down artificially and that causes this artificial leveling off of the cost to go so it's a it's a it's a bad approximation exactly in this corner and at this corner those are the ones that are trying to run off the side okay questions about that okay so oh yeah I worked hard on this and it's like so boring but to make that play you know and notebooks and work on binder and work on whatever yeah it's really slow you know so I should have had the input limit be like ten thousand or something actually I tried that and it overshoots because of the numeric so I'm sorry it's this point I tried the cool thing is you could you could have put other cost functions on there right so this minimum time cost this you know I'm accumulating cost everywhere until I get to the goal was one choice but there are other natural choices too this one says penalize me from being away from the goal so at every time step I'll get a score based on x squared how far am i from the origin okay and I'm also gonna penalize this time yeah I still got the input limits here but this time I'm going to penalize if you use it heart heart actions so I'd like to have a softer policy right I'd like to one that's not always riding the brakes and they're the accelerator okay the same algorithm does its thing it computes the cost to go right iterates iterates iterates and it comes up with this new optimal policy which now has intermediate come on come on it's gonna move in a minute there's a there's a slope there okay grant moved it to front there we go okay so there's a whole nice range in the middle where you're using non maximal actions okay but it still gets too little in fact the cost to go is you know it's kind of got the same kind of trough in fact the dynamics of that haven't changed there's still this this observation even with a different cost function that it's way better to be here than it is to be here right even if I'm taking a gentle approach if I'm here the passive dynamics of the system are tending to pull me this way okay if I'm here I have to go a long way around to get back that's true in the bangbang case but that's true more generally right the dynamics of wanna take me around in a spiral so this is gonna be closer this so the trough goes out he said you know asymmetrically so there well it's it's skewed this way right the trough that way and the this simple graph search algorithm somehow understands that this deeper idea of the dynamics okay so let's try it for the pendulum what is the optimal policy look like for the pendulum remember we said that it could be you know it could be just erase the vector field and going straight up but if you have torque limits which I'm going to put on here then you wouldn't expect that to work so what's a gentler policy for the pendulum think about it think about it okay I'll start with the minimum time cost right so I've got input saturations I've got an input limit of three sorry I didn't tell you what m and L and G were but relative to some unknown quantities three okay and I'm gonna call my exactly the same code all right which ones you want to look at first this is gonna take a minute for alright this is this is the this is I won't actually want to look at this first does that look familiar you see any eyeballs there the cost to go function it understands my pendulums dynamics it really sort of gets it right it says there's these eyeballs in my dynamics it's taking the short-term cost of being one everywhere zero in the goal and it's using the dynamics to diffuse that cost out and tell me one of the implications of being a long way given this non-trivial dynamical system right this is probably with a little bit of damping yeah I think yeah you can see it right which I'm happy to run it without actually that's a good tester this is still the bangbang version so it's riding the rails okay all the time it has a similar also for the pendulum it's still if your cost is if you have no cost on action but you have limits on action then in general you'll expect the optimal policy to be full on or full off bangbang is a sort of a standard thing okay but the switching between those two becomes much more subtle for the pendulum right there's some places here where you have to go around or whatever and it does its thing now this one that's yeah it's something right pretty good except it doesn't get to the top which is kind of a bummer that's my discritization error yet again okay and actually it's really really hard to get around that there are some things that that even you know people who spend their lives working on hjb methods for basically solving value duration really really well for heart problems with you know differential gains everything they still have choose with some of these interpolation problems it's actually very hard yeah yes that is part of it so so certainly you'd expect the optimal action well in the quadratic case it's completely that in the bangbang case it's a little less that you'd actually expect the solution to be finite time what's that oh it's gonna be on easy Tobey it's a shut up here you just ran out the p-set okay to be to be discovered on your own good intuition okay so let's see what happens in the quadratic regulator case where I've got a now a smoother gentler cost function by the way these codes run fast I slowed them down to anima to just you know plot all the intermediate solutions but it's not us it's not actually a slow algorithm the interface however sucks right you see my eyeball well the pendulums eyeball right that's that's a pretty good shot right you can see the characteristics of the eyeball right there okay and if I don't unsettled togeer too much let's see that one actually it was interesting the first one went more straight up and this one pumped a little bit to get up okay but it's actually backing out highly non-trivial solutions for this for this set of equations I'm gonna see if I can quickly set the damping to be 0 it's a little bit of code unfortunately but I'll make it an option for when it's on though when it's on the website I have to set the parameters a little bit awkwardly in that particular case good okay so let's think about what's happening here so we said you have minimal ability to change the vector field and you'd like to somehow remember this sort of I like to just do minimal control right and try to do highly non-trivial things we just gave our first algorithm that can do that with a few caveats what are the big caveat s' right one of them is accuracy right which you'll explore on the problems that apparently right this is from my descritization errors and they're sort of systemic they're there they're gonna it's not just like I get the answer wrong a little bit everywhere it's that they build up actually small errors near the goal for instance can cause characteristically wrong errors answers away from the goal and they're especially bad they're especially bad when you're working with systems that have disk discontinuities in the solution so the bang bang solution where you got this big rail right where the optimal solution is very different right next to it in neighboring states that's like the worst case for discrimination errors right because if you end up on just the wrong side of that you're gonna take exactly the wrong action right and so those those errors build up fast what are some other caveats here yeah awesome scalability this only works if I can make a course or a fine enough mesh over my state space and in practice people say you know dimensions like maybe around five six seven right when I started working out all this stuff people were saying oh we have four or five and then you know twenty some more years later it's like still five and it's exponential right so I mean computers got a lot faster and really that number hasn't it hasn't moved people have slightly better codes and they have refined meshing and stuff like that so some people will say six you know whatever but it's not suddenly twenty right this is an exponential person this is a curse of dimensionality this is the well-known bellman person dimensionality that it would take a discrete approximation an exponential number of nodes right every time I add a dimension I'm gonna have to multiply again all of my states that I had before multiply again so it's an exponentially bad number of states that has to do it so in practice actually I think people forget that there are if you can find a low dimensional system of sort of arbitrary complexity the same way for graphical analysis if it's on a line you can give me whatever function you want and I can understand everything about it right it's not about is it a sine wave as a cosine is that it got disk on duty I can tell you everything about a graphical and graphical analysis if it's on a line a flow on the line and just except for numerical issues right you can give me arbitrarily complex systems in a few dimension in a few variables and I can solve them pretty well with value iteration okay so that is the limit where we really do know how to do optimal control and we're gonna be able to throw in an uncertainty and all kinds of other things into this algorithm so it's it's very good the other limit to forecast again what's coming if something's sort of almost arbitrary dimension but linear again we're gonna be able to solve optimal control problems very wrong unfortunately all the interesting robots are sort of high dimensional and nonlinear right so we have to somehow grow from our ability to solve things at the extremes into this the much harder intermediate cases I would put one other major caveat here anybody guess it yeah good all right that's close yeah yeah so it assumes I know my model now you can imagine in the graph search world there's stochastic shortest path problems you can have some probability of transitioning on each edge so if you have uncertainty about your model parameters we're gonna be able to fit that in and we will when we get to stochastic optimal control okay so if you're you know you know most of your model but you're a little bit unsure what the friction is or something like that it's between you know something and something we're gonna be able to handle that kind of thing but it this does make an assumption about the model being known the one I think that's almost more more of a killer is state again okay true so I can ou know how to make salad or tie my shoes with this good cost function oh man there's getting to be a longer list good call oh is it to execute the controller that I get out of this I have to know what state I'm in that's the one that's driving me crazy these days and that's one that really the model class I mean people know about you know the partially observable versions of these problems but really we don't have super satisfying again in very small problems we can solve those okay but we're gonna see controllers that are beautiful and simple and good and work really well in on real systems that I don't know how to get out of a recipe from optimal control yet and I think the biggest reason in my book is that is this problem here well we'll talk more about that later okay good so I want you to have this sort of like very intuitive sense of what's happening with these optimization problems right if you can write an additive cost function then dynamic programming sort of is the crux of control it's talking about taking short term costs and spreading them out over the long term cost and it takes a very long like look ahead problem of what do I have to do for the next 30 seconds to say if I found a cost to go function all I have to do is try to go downhill and my cost to go function right and then it takes all the hard work out all the hard work is finding the cost to go function right but once I have it all I have to do is try to go down okay and that is just going to be such a core idea for control and I hope that this is like just your you know the first day of a sort of a new chapter in using optimization more generally for designing these controllers a lot of the discritization errors the numerical errors scaling errors that were coming up with you know they come down to having to do this as naively min over a by searching over all A's trying them all and taking the best one okay but you can imagine if I can make a conduct the continuous family of A's and I can do some smarter optimization over that then I'm gonna be able to scale better okay and we're gonna take that journey together over the next few lectures see you next time problem sets due tomorrow all right
</chapter>

</body>
</html>
