<!DOCTYPE html>
<html>
<head>
  <title>Maximum A Posteriori (MAP) estimation</title>
  <meta name="Maximum A Posteriori (MAP) estimation" content="text/html; charset=utf-8;" />
  <script type="text/javascript" src="../../../logbook.js"></script>

  <script src="../../../logbook-mathjax-config.js" defer></script> 
  <script type="text/javascript" id="MathJax-script" defer
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/atom-one-light.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.4.0/mermaid.min.js"></script>
  <script> mermaid.initialize({startOnLoad: true}); </script>  

  <link rel="stylesheet" type="text/css" href="../../../logbook.css" />
</head>

<body onload="loadChapter('');">  

  <div data-type="titlepage" pdf="no">
    <header>
      <h1><a href="../../../index.html" style="text-decoration:none;">Logbook</a></h1>
      <p style="font-size: 18px;"><a href="../../../bio.html">Jayson Wynne-Thomas</a></p>
      <p style="font-size: 14px; text-align: right;"> 
        Last modified <span id="last_modified"></span>.</br>
        <script>
        var d = new Date(document.lastModified);
        document.getElementById("last_modified").innerHTML = d.getFullYear() + "-" + (d.getMonth()+1) + "-" + d.getDate();</script>
      </p>
    </header>
  </div>
  
  <div id="main" class="sidebar1">
    <span style="font-size:10px;cursor:pointer" onclick="openNav()">&#9776;</span>
  </div>

  <div id="mySidenav" class="sidebar">
  
<a href="#0">Description</a>
<a href="#1">Finding $p(x|y)$</a>
<a href="#2">Choosing $\hat{x}$</a>
<a href="#3">Musings</a>
</div>

<chapter style="counter-reset: chapter 0"><h1>Maximum A Posteriori (MAP) estimation</h1>

<section id="0"><h1>Description</h1>
  [<a href="https://t.ly/sX-Rw">Link to explore</a>, Might be a <a href="https://t.ly/jKjhu">better link</a>]
  
  <p>
    Let's assume that we have a <i>prior estimate</i> of a vehicle position, <n>given as a Gaussian</n> with an initial estimate $\hat{x}_0$ and covariance $P_0$. The subscript $0$ denotes the estimate before any measurement is received and subscript $1$ denotes the estimate after one measurement is received.
  </p>

  $$
    p(x) \sim \mathcal{N}(\hat{x}_0, P_0)
  $$

  How can we actually use this prior in our estimation process? We adopt a very similar approach to the two steps we did before maximum likelihood estimation:

  <ul>
    <li>We first found the probability density function $p(y|x)$</li>
    <li>We selected $x$ to be the value that yielded the maximum likelihood of the measurements, $y$ given $\hat{x}$</li>
  </ul>

  Now that we have a prior, we can find the posterior estimate $p(x|y)$. The two steps of the new process are:

  <ul>
    <li>Find the probability density function $p(x|y)$</li>
    <li>Select $\hat{x}$ to be the value that yields the maximum $p(\hat{x}|y)$</li>
    <p>
      The output of this recursive estimate is usually referred to as the <i>Maximum A Posteriori (MAP) estimate</i>. We turn the prior into a posterior using new information from the new measurement. 
    </p>
  </ul>
</section>

<section id="1"><h1>Finding $p(x|y)$</h1>
  Applying Bayes' rule to Gaussians, we can get:
  $$\begin{align*}
    p(x|y) &= \frac{p(y|x)p(x)}{p(y)} \\
    &= \underbrace{\alpha}_{\text{Normaliser}}
    \underbrace{\mathcal{N}(Hx, R)}_{p(y|x)}
    \underbrace{\mathcal{N}(\hat{x}_0, P_0)}_{p(x)}
  \end{align*}$$
  
  $p(x)$ is the prior. We can ignore the normaliser alpha just as we did in the maximum likelihood estimate, as it is not going to have any impact on the maximum.
</section>

<section id="2"><h1>Choosing $\hat{x}$</h1>
  We select $\hat{x}$ hat to be the value that yields a maximum likelihood of $p(\hat{x}|\tilde{y})$. If we look at the full formula for the product Gaussian:

  $$
    p(\hat{x}|\tilde{y}) = 
    \underbrace{
    \frac{1}
    {(2\pi)^{\frac{n}{2}} \lvert{R}\rvert^{\frac{1}{2}}}
    \frac{1}
    {(2\pi)^{\frac{n}{2}} \lvert{P_0}\rvert^{\frac{1}{2}}}
    }_{\text{Normalisers}}

    e^\underbrace{{
      -\frac{1}{2}(\tilde{y} - H\hat{x})^T 
      R^{-1}(\tilde{y} - H\hat{x}) 
      - \frac{1}{2}(\hat{x} - \hat{x}_0)^T
      P_0^{-1}(\hat{x}-\hat{x}_0)
    }}_{\qquad\qquad\qquad\text{Maximise}}
  $$

  We have 2 of the normalizers from the Gaussian. We also have the product of the two exponents, which when we take the product of exponents becomes the exponent of the sum.

  <p>
    Just as before, the normaliser term does not depend on $x$, whereas the exponent does. We take the maximum of the function in the exponent by taking the derivative and finding where it's zero.
  </p>

  The result for this maximum A posteriori estimate is still a Gaussian with an updated mean and covariance:

  $$
    p(\hat{x}|\tilde{y}) \sim \mathcal{N}(\hat{x}_1, P_1)
  $$

  <button class="accordion">Steps of MAP</button>
  <div class="panel">
    <p></p>
    <ul>
      <li>We first do a covariance update to get $P_1$</li>
      $$
        P_1 = (P_0^{-1} + H^TR^{-1}H)^{-1}
      $$
      $P_1$ depends on $R$ even if $R$ is defined to be a variance times the identity matrix.

      <p>
        <li>The mean is then updated based on the new covariance</li>
        $$
          \hat{x} = \hat{x}_0 + 
          \underbrace{P_1}_{\text{Prior} \atop \text{covariance}}
          H^T
          \underbrace{R^{-1}}_{\text{Measurement} \atop \text{covariance}}
          (\tilde{y} - H\hat{x}_0)
        $$
      </p>
    </ul> 
  </div>
  The MAP estimate $\hat{x}$ is a weighted average between the prior $\hat{x}_0$ and what the new measurement says about $x$, $(\tilde{y} - H\hat{x}_0)$. The weights come from the relative magnitude of the prior and the measurement covariance.

  <p>
    The more we trust the prior estimate, the smaller the prior covariance! And the less the measurements will affect the estimate. The more we trust the measurements, the smaller the measurement covariance and the more the new measurements will change the current estimate.
  </p>

  A balance between prior and new information is the heart of the estimation process.
</section>

<section id="3"><h1>Musings</h1>
  <ul>
    <li>Why is $p(y)$ discounted</li>
    <li>What is the derivation of the exponent term when we try to maximise it.</li>
  </ul>
</section>
</chapter>

</body>
</html>
