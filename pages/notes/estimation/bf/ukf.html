
<!DOCTYPE html>
<html>
<head>
  <title>Unscented Kalman Filter (UKF)</title>
  <meta name="Unscented Kalman Filter (UKF)" content="text/html; charset=utf-8;" />
  <script type="text/javascript" src="../../../../logbook.js"></script>

  <script src="../../../../logbook-mathjax-config.js" defer></script> 
  <script type="text/javascript" id="MathJax-script" defer
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/atom-one-light.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.4.0/mermaid.min.js"></script>
  <script> mermaid.initialize({startOnLoad: true}); </script>  

  <link rel="stylesheet" type="text/css" href="../../../../logbook.css" />
</head>

<body onload="loadChapter('');">  

  <div data-type="titlepage" pdf="no">
    <header>
      <h1><a href="../../../../index.html" style="text-decoration:none;">Logbook</a></h1>
      <p style="font-size: 18px;"><a href="../../../../bio.html">Jayson Wynne-Thomas</a></p>
      <p style="font-size: 14px; text-align: right;"> 
        Last modified <span id="last_modified"></span>.</br>
        <script>
        var d = new Date(document.lastModified);
        document.getElementById("last_modified").innerHTML = d.getFullYear() + "-" + (d.getMonth()+1) + "-" + d.getDate();</script>
      </p>
    </header>
  </div>
  
  <div id="main" class="sidebar1">
    <span style="font-size:10px;cursor:pointer" onclick="openNav()">&#9776;</span>
  </div>

  <div id="mySidenav" class="sidebar">
  
<a href="#0">General concept</a>
<a href="#1">Sigma points</a>
<ul class="no-bullets">
  <li><a href="#1.0">Creating the sigma points from the distribution</a></li>
  <li><a href="#1.1">Recovering the distribution from the sigma points</a></li>
  <li><a href="#1.2">Weights</a></li>
</ul>
<a href="#2">The algorithm</a>
<ul class="no-bullets">
  <li><a href="#2.0">UKF Predict</a></li>
  <li><a href="#2.1">UKF update</a></li>
</ul>
<a href="#3">Advantages of UKF over EKF</a>
</div>

<chapter style="counter-reset: chapter 0"><h1>Unscented Kalman Filter (UKF)</h1>

<section id="0"><h1>General concept</h1>
  Instead of linear approximating the $g$ or $h$ function, UKF approximates the probability distribution itself. Given a distribution, UKF selects a small set of weighted sample points called <i>sigma points</i>, <n>such that the mean and covariance of these weighted points is equal to the mean and covariance of the distribution</n>.

  <div class="container">
    <figure>
      <img style="height:180px; width:auto"
      src="../../../../figures/drone/38_sigmaPoints.png"/>
      <figcaption>
        Sigma points
      </figcaption>
    </figure>
  </div>

  UKF propagates these small set of weighted points through the non-linear $g$ or $h$ function. We then use the mean and covariance of the transformed sigma points as the mean and covariance of the new state estimate. We can't have many sigma points as it might be computationally expensive. The real magic behind the UKF is what's called the <i>Unscented Transform</i>, and the first step of the Unscented Transform is getting these special sigma points. 

</section>

<section id="1"><h1>Sigma points</h1>
  <subsection id="1.0"><h1>Creating the sigma points from the distribution</h1>
    We can't just symmetrically sample any points around the mean. In an $N$ dimensional state space, we need to generate $2N+1$ sigma points. The first sigma point is the mean. In 1D, the remaining 2 points might be $\mu - \sigma$ and $\mu + \sigma$, where $\sigma$ is the standard deviation.
    
    <p>
      In higher dimensions, $S$, the square root of the covariance matrix is computed. Since covariance is an $N \times N$ matrix, $S$ becomes an $N \times 1$ matrix:

      $$
        S = \sqrt{\Sigma_t}
      $$
      $S_i$ is the $i^\text{th}$ row of $S$. $S$ is similar to a multidimensional version of a standard deviation in one dimension. 
    </p>

    The sigma points are computed as below:
    $$
      X_{i,t} = 
      \left\{
        \begin{array}{ c l }
          &= \mu_t, & \quad & i=0 \\
          &= \mu_t + \gamma S_i, & \quad & i=1,...,N \\
          &= \mu_t - \gamma S_{i-N}, & \quad & i=N+1,...,2N
        \end{array}
      \right.
    $$
    It means, the 1st point is taken as the mean. The next $N$ points should be sampled by stepping some distance away from the mean. The size of the step is set by $\gamma$ and the direction is given by row $i$ of the $S$ matrix. Then do the same thing in the opposite direction to make the sampling symmetric.

  </subsection>
  
  <subsection id="1.1"><h1>Recovering the distribution from the sigma points</h1>
    The mean is recovered by taking the weighted average of each sigma point:
    $$
      \mu = \sum^{2N}_{i=0} w^m_i X_i
    $$
    $w^m_i$ is the weight of point $X_i$.

    <p>
      The covariance is recovered by taking the weighted sample covariance of each sigma point:
      $$
        \Sigma = \sum^{2N}_{i=0} w^c_i (X_i - \mu)(X_i - \mu)^T
      $$
      $w^c_i$ is the weight of each sample.
    </p>
  </subsection>

  <subsection id="1.2"><h1>Weights</h1>
    Not all sigma points are considered equal. The weights for the mean calculation ($w^m_i$) are different from those for the covariance ($w^c_i$).

    $$
      w^m_i = 
      \left\{
        \begin{array}{ c l }
          &= \frac{\lambda}{N + \lambda}, & \quad & i=0 \\
          &= \frac{1}{2(N + \lambda)}, & \quad & i=1,...,2N
        \end{array}
      \right.
    $$

    $$
      w^c_i = 
      \left\{
        \begin{array}{ c l }
          &= \frac{\lambda}{N + \lambda} + (1 - \alpha^2 + \beta) \\
          &= \frac{1}{2(N + \lambda)}, & \quad & i=1,...,2N
        \end{array}
      \right.
    $$
  </subsection>
</section>

<section id="2"><h1>The algorithm</h1>
  <subsection id="2.0"><h1>UKF Predict</h1>
    First, we convert the mean and covariance into a set of sigma points, each of which are then propagated forward in time (transformed) using the state transition function $g$. The inputs to the predict function are still the same as the others. The output however is the expected set of sigma points.

    $$\begin{align*}
      \bar{X}_t &= f_{predict}(\mu_{t-1}, \Sigma_{t-1}, u_t, \Delta{t}):\\
      \\
      X_{t-1} &= f_{computeSigmas}(\mu_{t-1}, \Sigma_{t-1}) \\
      \forall^{2N}_{i=0} \quad \bar{X}_{i,t} &= g(X_{i,t-1}, u_t, \Delta{t})      
    \end{align*}$$
  </subsection>

  <subsection id="2.1"><h1>UKF update</h1>
    $$
      \mu_t, \Sigma_t = f_{update}(\bar{X}_t, z_t)
    $$
    
    <ul>
    <li>
      Pass the sigma points corresponding to the estimated state through the observation/measurement function to get ones corresponding to the expected observation
    </li>

    $$
      \forall^{2N}_{i=0} \quad Z_{i,t} = h(\bar{X}_{i,t})      
    $$

    <li>
      Recover the mean and covariance from the different sigma points:
    </li>

    $$\begin{align*}
      \bar{\mu}_t &= \sum^{2N}_{i=0} w^m_i \bar{X}_{i,t} \\
      \bar{\Sigma}_t &= \sum^{2N}_{i=0} w^c_i 
                        (\bar{X}_{i,t} - \bar{\mu}_t)
                        (\bar{X}_{i,t} - \bar{\mu}_t)^T + Q_t \\
      \\
      \mu^z_t &= \sum^{2N}_{i=0} w^m_i Z_{i,t} \\
      \Sigma^z_t &= \sum^{2N}_{i=0} w^c_i 
                        (Z_{i,t} - \mu^z_t)
                        (Z_{i,t} - \mu^z_t)^T + R_t \\

    \end{align*}$$

    <li>
      Compute the cross-covariance between the sigma points corresponding to the estimated state and the expected observation to know how they vary w.r.t. each other
    </li>
    $$
      \Sigma^{xz}_t = \sum^{2N}_{i=0} w^c_i 
      (\bar{X}_{i,t} - \bar{\mu}_t)
      (Z_{i,t} - \mu^z_t)^T
    $$

    <li>Compute the Kalman Gain</li>
    $$
      K_t = \Sigma^{xz}_t (\Sigma^z_t) ^{-1}
    $$

    <li>Compute the new mean and covariance</li>
    The mean is computed by adding a correction (difference between actual and expected measurement, multiplied by the kalman gain) to the predicted mean.
    $$\begin{align*}
      \mu_t &= \bar{\mu}_t + K_t(z_t - \mu^z_t) \\
      \Sigma_t &= \bar{\Sigma}_t - K_t\Sigma^z_tK_t^T
    \end{align*}$$
  </subsection>
</section>

<section id="3"><h1>Advantages of UKF over EKF</h1>
  <ul>
    <li>No derivatives, so easier to implement</li>
    For EKF, we had to take derivatives to compute the jacobian matrix. For UKF, all we need is a $g$ and an $h$ function. 

    <p>
    <li>Gives a better estimate</li>
    </p>

    <li>Is quicker</li>
  </ul>
</section>
</chapter>

</body>
</html>
