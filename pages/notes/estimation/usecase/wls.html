
<!DOCTYPE html>
<html>
<head>
  <title>Weighted Linear Least Squares</title>
  <meta name="Weighted Linear Least Squares" content="text/html; charset=utf-8;" />
  <script type="text/javascript" src="../../../../logbook.js"></script>

  <script src="../../../../logbook-mathjax-config.js" defer></script> 
  <script type="text/javascript" id="MathJax-script" defer
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/atom-one-light.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <link rel="stylesheet" type="text/css" href="../../../../logbook.css" />
</head>

<body onload="loadChapter('');">  

  <div data-type="titlepage" pdf="no">
    <header>
      <h1><a href="../../../../index.html" style="text-decoration:none;">Logbook</a></h1>
      <p style="font-size: 18px;"><a href="../../../../pages/bio/jjwt.html">Jayson Wynne-Thomas</a></p>
      <p style="font-size: 14px; text-align: right;"> 
        Last modified <span id="last_modified"></span>.</br>
        <script>
        var d = new Date(document.lastModified);
        document.getElementById("last_modified").innerHTML = d.getFullYear() + "-" + (d.getMonth()+1) + "-" + d.getDate();</script>
      </p>
    </header>
  </div>
  
  <div id="main" class="sidebar1">
    <span style="font-size:10px;cursor:pointer" onclick="openNav()">&#9776;</span>
  </div>

  <div id="mySidenav" class="sidebar">
  
<a href="#0">Estimating multiple unknowns</a>
<a href="#1">Defining the cost function</a>
<a href="#2">Minimising the cost function</a>
<a href="#3">Example</a>
<a href="#4">Summary of WLS vs LS</a>
</div>

<chapter style="counter-reset: chapter 0"><h1>Weighted Linear Least Squares</h1>
<section id="0"><h1>Estimating multiple unknowns</h1>
  <p>
    From now on, let's drop the assumption that we're only estimating one parameter and derive the more general normal equations. This will allow us to formulate a way to estimate multiple parameters at one time. For example, if we wanted to estimate several resistance values at once. 
  </p>

  Let's begin by using the following general notation. We have $m$ 
  measurements that are related to $n$ unknown parameters through a linear model represented by $H$, the Jacobian matrix whose form and entries will depend on the particular problem at hand. 

  $$
    \begin{bmatrix}y_1 \\ . \\ . \\ y_m\end{bmatrix}
    = H\begin{bmatrix}x_1 \\ . \\ . \\ x_n\end{bmatrix}
    + \begin{bmatrix}v_1 \\ . \\ . \\ v_m\end{bmatrix}
  $$
  $$
    y = Hx + v
  $$
</section>
  
<section id="1"><h1>Defining the cost function</h1>
  What if some of our measurements are of better quality than others i.e. there's varying measurement noise variance? For example, 1 sensor is better than the other.

  One way to interpret the ordinary method of least squares is to say 
  that we are implicitly assuming that each noise term $v_i$ is an independent random variable across measurements and has an equal variance i.e. IID.

  $$
    \mathbb{E}[v^2_i] = \sigma^2\enspace (i=1,..,m)
    \qquad \qquad R = \mathbb{E}[vv^T]=
    \begin{bmatrix}
    \sigma^2 & & 0 \\
    & \ddots & \\
    0 & & \sigma^2    
    \end{bmatrix}
  $$
  
  $\mathbb{E}$ is the expected value. The expected value of the square of the deviation from the mean (here, the mean is assumed to be the true value) is the same as the variance.
  
  <p>
    If we instead assume that each noise term is independent but has a
    different variance, we can define our noise covariance as follows.
  </p>

  $$
    \mathbb{E}[v^2_i] = \sigma^2_i \enspace (i=1,..,m)
    \qquad \qquad R = \mathbb{E}[vv^T]=
    \begin{bmatrix}
    \sigma_1^2 & & 0 \\
    & \ddots & \\
    0 & & \sigma_m^2    
    \end{bmatrix}
  $$

  We can define a weighted least squares criterion as:
  $$\begin{align*}
    \mathcal{L}_{WLS}(x) &= e^TR^{-1}e\\
    &= \frac{e^2_1}{\sigma^2_1} + \cdots + \frac{e^2_m}{\sigma^2_m}
  \end{align*}$$
  
  Each squared error term is now weighted by the inverse of the variance 
  associated with the corresponding measurement. In other words, the lower the variance of the noise, the more strongly it's associated error term will be weighted in the loss function. 
  
  <p>
    In the case of equal variances, we get $\frac{1}{\sigma^2}$; but since the variance is constant, it will not affect the final estimate. 
  </p>
</section>
    
<section id="2"><h1>Minimising the cost function</h1>
  Expanding the new criterion:

  $$\begin{align*}
    \mathcal{L}_{WLS}(x) &= e^TR^{-1}e\\
    &= (y-Hx)^TR^{-1}(y-Hx)
  \end{align*}$$

  We minimise the same way as before by taking a derivative. 
  In the general case where we have $n$ unknown parameters in our bold vector $x$, 
  this derivative will actually be a gradient. Setting the gradient to the 0 vector, 
  we then solve for our best or optimal parameter vector $\mathbf{\hat{x}}$. 
  $$
    \hat{x} = argmin_x\mathcal{L}(x) \qquad \longrightarrow \qquad
    \frac{\partial{\mathcal L}}{\partial x}\Bigr|_{x=\hat{x}}
    = 0 = -y^TR^{-1}H + \hat{x}^TH^TR^{-1}H
  $$
  This leads to another set of normal equations called the weighted normal equations. 
  $$\begin{align*}
    H^TR^{-1}H\hat{x}_{WLS} &= H^TR^{-1}y \\
    \hat{x} &= (H^TR^{-1}H)^{-1} H^TR^{-1}y
  \end{align*}$$

  An individual variance is assigned to each measurement, which means that the 
  matrix $\mathbf{R}$ (and its inverse) is $m \times m$ in size.
</section>

<section id="3"><h1>Example</h1>
  <figure>
    <img style="height:70px; width:auto"
    src="../../../figures/drone/19_wlsExample.png"/>
  </figure>

  Noise is described in standard deviations, hence why they have the units of ohms. They need to be squared to get the variances.

  $$\begin{align*}
    \hat{x} &= (H^TR^{-1}H)^{-1} H^TR^{-1}y \\
    &=
    \begin{bmatrix}
      \begin{bmatrix}1 & 1 & 1 & 1\end{bmatrix} &
      \begin{bmatrix}400&&& \\ &400&& \\ &&4& \\ &&&4 \end{bmatrix}^{-1} &
      \begin{bmatrix}1 \\ 1 \\ 1 \\ 1\end{bmatrix}
    \end{bmatrix}^{-1} 

    \begin{bmatrix}1 & 1 & 1 & 1\end{bmatrix}
    \begin{bmatrix}400&&& \\ &400&& \\ &&4& \\ &&&4 \end{bmatrix}^{-1}
    \begin{bmatrix}1068 \\ 988 \\ 1002 \\ 996\end{bmatrix} \\

    &= \frac{1}{1/400 + 1/400 + 1/4 + 1/4}
    (\frac{1068}{400} + \frac{988}{400} + \frac{1002}{4} + \frac{996}{4}) \\
    &= 999.3\Omega
  \end{align*}$$

  Evaluating the weighted least squares solution, we can see that the 
  final resistance value is much closer to what the more accurate multimeter 
  measured.
</section>
  
<section id="4"><h1>Summary of WLS vs LS</h1>
  By using weighted least squares, we can vary the importance of each measurement to the final estimate. Measurements can have different   variances and also, sometimes, be correlated. 

  <p>
    <table class="table2 center">
      <tr>
        <th></th>
        <th>LSs</th>
        <th>WLSs</th>
      </tr>
      <tr>
        <td>Loss/Criterion</td>
        <td>$\mathcal L_{LS}(x) = e^Te$</td>
        <td>$\mathcal L_{WLS}(x) = e^TR^{-1}e$</td>
      </tr>
      <tr>
        <td>Solution</td>
        <td>$\hat{x}_{LS} = (H^TH)^{-1}H^Ty$</td>
        <td>$\hat{x}_{WLS} = (H^TR^{-1}H)^{-1}H^TR^{-1}y$</td>
      </tr>
      <tr>
        <td>Limitations</td>
        <td>$m\geq n$</td>
        <td>
          $m \geq n$ <br>
          $\sigma^2_i > 0$
        </td>
      </tr>
    </table>
  </p>

  We derived the weighted least squares criterion and the associated weighted normal equations that can be solved to yield the weighted least squares estimate of a set of 'constant parameters'. We need to modify the method of least squares to work recursively, that is to compute an optimal estimate based on a stream of measurements without having to acquire the entire set beforehand, when we look at state estimation, or the problem of estimating quantities that change continuously over time.
</section>
</chapter>

</body>
</html>
