<!DOCTYPE html>
<html>
<head>
  <title>Gain matrix</title>
  <meta name="Gain matrix" content="text/html; charset=utf-8;" />
  <script type="text/javascript" src="../../../../../../logbook.js"></script>

  <script src="../../../../../../logbook-mathjax-config.js" defer></script> 
  <script type="text/javascript" id="MathJax-script" defer
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/atom-one-light.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.4.0/mermaid.min.js"></script>
  <script> mermaid.initialize({startOnLoad: true}); </script>  

  <link rel="stylesheet" type="text/css" href="../../../../../../logbook.css" />
</head>

<body onload="loadChapter('');">  

  <div data-type="titlepage" pdf="no">
    <header>
      <h1><a href="../../../../../../index.html" style="text-decoration:none;">Logbook</a></h1>
      <p style="font-size: 18px;"><a href="../../../../../../bio.html">Jayson Wynne-Thomas</a></p>
      <p style="font-size: 14px; text-align: right;"> 
        Last modified <span id="last_modified"></span>.</br>
        <script>
        var d = new Date(document.lastModified);
        document.getElementById("last_modified").innerHTML = d.getFullYear() + "-" + (d.getMonth()+1) + "-" + d.getDate();</script>
      </p>
    </header>
  </div>
  
  <div id="main" class="sidebar1">
    <span style="font-size:10px;cursor:pointer" onclick="openNav()">&#9776;</span>
  </div>

  <div id="mySidenav" class="sidebar">
  
<a href="#0">Cost function</a>
<a href="#1">Minimising the cost function</a>
<a href="#2">Derivatives of matrix traces</a>
</div>

<chapter style="counter-reset: chapter 0"><h1>Gain matrix</h1>

<section id="0"><h1>Cost function</h1>
  The cost function is the sum of variances of the estimation errors, i.e. the <i>trace</i> of the estimation error covariance matrix.

  $$\begin{align*}
    W_k &= tr(P_k) \\
        &= E[\epsilon^2_{1,k}] + E[\epsilon^2_{2,k}] + \cdots + E[\epsilon^2_{n,k}]
  \end{align*}$$

  Using the expanded version of the estimation error covariance propagation equation from <a href="estCovPropEq.html#1">here</a>, the cost function can be re-written as:
  $$\begin{alignat*}{2}
    W_k &=\, && tr(P_k) \\
        &=\, && tr(P_{k-1}) - tr(K_kH_kP_{k-1}) - tr(P_{k-1}H_k^TK_k^T) +\\
        & \, && tr(K_kH_kP_{k-1}H_k^TK_k^T) + tr(K_kR_kK^T_k) \\
  \end{alignat*}$$
</section>

<section id="1"><h1>Minimising the cost function</h1>
  Gain matrix $K_k$ is found by minimising the cost function, i.e. by solving:
  $$
    \frac{\partial W_k}{\partial K_k} = 0
  $$

  <p>
    Taking the derivative with respect to $K$, helps determine how the cost function $W$ changes when $K$ is adjusted. Setting the derivative to $0$ finds the $K$ that leads to the smallest possible value of $W$.
  </p>

  Using expressions from <a href="#2">3</a>
  $$
    \frac{\partial W_k}{\partial K_k} = -2P_{k-1}H_k^T + 2H_kP_{k-1}H_k^TK_k + 2K_kR_k = 0
  $$

  Rearranging:
  $$\begin{align*}
    &K_k(H_kP_{k-1}H_k^T + R_k) = P_{k-1}H_k^T \\
    &K_k = P_{k-1}H_k^T(H_kP_{k-1}H_k^T + R_k)^{-1}
  \end{align*}$$
</section>

<section id="2"><h1>Derivatives of matrix traces</h1>
  [<a href="https://t.ly/1hwWV">Section 2.5 in the matrix cookbook</a>] <br>
  Since $P_{k-1}$ is a constant:
  $$\begin{align*}
    \frac{\partial{tr(P_{k-1})}}{\partial{K_k}} &= 0 \\\\

    \frac{\partial{tr(AX^TB)}}{\partial{X}} &= BA \\
    \frac{\partial{tr(P_{k-1}H_k^TK_k^T)}}{\partial{K_k}} &= P_{k-1}H_k^T
  \end{align*}$$

  Since the estimation and measurement noise covariance matricees are symmetric, $P_k^T = P_k$ and $R_k^T = R_k$.
  $$\begin{align*}
    \frac{\partial{tr(XBX^T)}}{\partial{X}} &= XB^T + XB \\
    \frac{\partial{tr(K_kH_kP_{k-1}H_k^TK_k^T)}}{\partial{K_k}} &= 2H_kP_{k-1}H_k^TK_k \\
    \frac{\partial{tr(K_kR_kK^T_k)}}{\partial{K_k}} &= 2K_kR_k \\\\       

    \frac{\partial{tr(AXB)}}{\partial{X}} &= A^TB^T \\
    \frac{\partial{tr(H_kP_{k-1}K_k)}}{\partial{K_k}} &= P_{k-1}H_k^T
  \end{align*}$$
</section>
</chapter>

</body>
</html>
