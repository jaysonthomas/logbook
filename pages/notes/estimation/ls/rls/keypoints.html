
<!DOCTYPE html>
<html>
<head>
  <title>RLS: Key points</title>
  <meta name="RLS: Key points" content="text/html; charset=utf-8;" />
  <script type="text/javascript" src="../../../../../logbook.js"></script>

  <script src="../../../../../logbook-mathjax-config.js" defer></script> 
  <script type="text/javascript" id="MathJax-script" defer
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/atom-one-light.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.4.0/mermaid.min.js"></script>
  <script> mermaid.initialize({startOnLoad: true}); </script>  

  <link rel="stylesheet" type="text/css" href="../../../../../logbook.css" />
</head>

<body onload="loadChapter('');">  

  <div data-type="titlepage" pdf="no">
    <header>
      <h1><a href="../../../../../index.html" style="text-decoration:none;">Logbook</a></h1>
      <p style="font-size: 18px;"><a href="../../../../../bio.html">Jayson Wynne-Thomas</a></p>
      <p style="font-size: 14px; text-align: right;"> 
        Last modified <span id="last_modified"></span>.</br>
        <script>
        var d = new Date(document.lastModified);
        document.getElementById("last_modified").innerHTML = d.getFullYear() + "-" + (d.getMonth()+1) + "-" + d.getDate();</script>
      </p>
    </header>
  </div>
  
  <div id="main" class="sidebar1">
    <span style="font-size:10px;cursor:pointer" onclick="openNav()">&#9776;</span>
  </div>

  <div id="mySidenav" class="sidebar">
  
<a href="#0">Makes recursive incremental updates</a>
<a href="#1">Minimises the variance of the unknowns at every time step</a>
<a href="#2">Forms the update step of the linear Kalman filter</a>
</div>

<chapter style="counter-reset: chapter 0"><h1>RLS: Key points</h1>

<section id="0"><h1>Makes recursive incremental updates</h1>
  RLS is a recursive linear estimator where in an estimate is updated incrementally as new measurements stream in. 

  <p>
    RLS extends the batch LS solution to work on the fly by:
    <ul>
      <li><n>Keeping a running optimal estimate</n> (the LS solution) of some unknown parameter/state for all measurements collected up to the previous time step.</li>
      <li>
        <n>Updating the optimal estimate</n> given the measurement at the current time step.
      </li>
    </ul>
  </p>

  <subsubsection><h1>Mathematical expression</h1>
    A linear recursive estimate $\hat{x}$, at time $k$, is a linear combination of:
    <ul>
      <li>The previous estimate (best guess so far) and</li>
      <li>The <i>current measurement residual</i> (error difference between expected and the actual measurement) <n>weighted</n> by a gain matrix $K_k$</li>
    </ul>
  
    $$\begin{equation} \label{eq:linearRecursiveFormulation}
      \hat{x}_k = \hat{x}_{k-1} + 
      \underbrace{K_k}_{\text{Estimator} \atop \text{gain matrix}}
      \underbrace{(y_k - H_k\hat{x}_{k-1})}_{\text Innovation}
    \end{equation}$$

    The residual quantifies how well the current measurement matches the previous best estimate. <n>If it is $0$, the old estimate does not need to be updated</n>.
  </subsubsection>
</section>
  
<section id="1"><h1>Minimises the variance of the unknowns at every time step</h1>
  Instead of minimising the error directly, RLS uses a probabilistic formulation to minimise the <n>expected value</n> of the squared measurement residual i.e. the variance, at every time step. The lower the variance, the more certain the estimate becomes.

  <p>
    For a single scalar parameter, the cost function is the variance. At time $k$, it is:
  </p>
  $$\begin{align*}
    \mathcal L_{RLS} &= \mathbb{E}[(x - \hat{x}_k)^2] \\
    &= \sigma^2_k
  \end{align*}$$

  For <n>multiple $n$ unknown parameters</n>, the cost function is the trace of the covariance matrix. At time step $k$, it is: 
  
  $$\begin{align*}
    \mathcal L_{RLS} &= \mathbb{E}[(x_1 - \hat{x}_{1k})^2
    + \cdots + (x_n - \hat{x}_{nk})^2] \\
    &= Trace(\underbrace{P_k}_{\text Estimator \atop \text covariance})
  \end{align*}$$
</section>

<section id="2"><h1>Forms the update step of the linear Kalman filter</h1>
  Using the linear recursive formulation (\ref{eq:linearRecursiveFormulation}), the covariance matrix $P_k$ can be expressed as a function of $K_k$:

  $$
    P_k = (1-K_kH_k)P_{k-1}(1-K_kH_k)^T + K_kR_kK^T_k
  $$  
  
  The cost function of RLS is minimised when its derivative w.r.t. $K_k$ is equal to $0$. This relationship leads to taking derivatives of matrix traces of the various terms in $P_k$ which leads to $K_k$:

  $$
    K_k = P_{k-1}H^T_k(H_kP_{k-1}H^T_k + R_k)^{-1}
  $$
  
  Finally, by using this formulation, the recursive definition for $P_k$ can be further simplified: 

  $$\begin{align*}
    P_k &= P_{k-1} - K_kH_kP_{k-1} \\
    &= (I-K_kH_k)P_{k-1}
  \end{align*}$$
  
  <m>The larger the gain matrix $K$, the smaller the new estimator covariance will be</m>.
</section>
</chapter>

</body>
</html>
